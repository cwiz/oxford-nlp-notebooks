{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use Pride and Pejustice as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset if it's not already there\n",
    "if not os.path.isfile('pride-and-prejustice.txt'):\n",
    "    urllib.request.urlretrieve(\"https://www.gutenberg.org/files/1342/1342-0.txt\", filename=\"pride-and-prejustice.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = open(\"pride-and-prejustice.txt\", \"r\", encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ful property\n",
      "of some one or other of their daughters.\n",
      "\n",
      "“My dear Mr. Bennet,” said his lady to him one day, “have you heard that\n",
      "Netherfield Park is let at last?”\n",
      "\n",
      "Mr. Bennet replied that he had not.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(txt[1000:1200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert text into list of wors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "words = re.split('[^A-Za-z]+', txt.lower())\n",
    "words = list(filter(None, words)) # Remove empty strings\n",
    "sentences = re.split('(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', txt.lower())\n",
    "sentences = filter(None, sentences) # Remove empty string\n",
    "mapfn = lambda item: re.sub(r\"[^a-z0-9]+\", \" \", item.lower())\n",
    "sentences = list(map(mapfn, sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6530\n",
      "['takes', 'cruel', 'observe', 'mixed', 'kitty', 'compromised', 'sweetness', 'dishes', 'inferiority', 'importing', 'patroness', 'vexations', 'laconic', 'produced', 'ensuring', 'originator', 'eager', 'redistributing', 'openly', 'method']\n"
     ]
    }
   ],
   "source": [
    "# Create set of all unique words, this throws away any information about frequency however\n",
    "gram1 = set(words)\n",
    "\n",
    "print(len(gram1))\n",
    "\n",
    "# Instead of printing all the elements in the set, create an iterator and print 20 elements only\n",
    "gram1_iter = iter(gram1)\n",
    "print([next(gram1_iter) for _ in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subscribe to\n",
      "to our\n",
      "our email\n",
      "email newsletter\n",
      "newsletter to\n",
      "to hear\n",
      "hear about\n",
      "about new\n",
      "new ebooks\n"
     ]
    }
   ],
   "source": [
    "# See the last 10 pairs\n",
    "for i in range(len(words)-10, len(words)-1):\n",
    "    print(words[i], words[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125900\n",
      "55640\n",
      "[('had', 'taken'), ('contradicted', 'your'), ('and', 'arranged'), ('whether', 'all'), ('painful', 'confusion'), ('not', 'sink'), ('almost', 'promised'), ('spirits', 'as'), ('violates', 'the'), ('they', 'looked'), ('bingley', 'but'), ('exaggeration', 'the'), ('could', 'encourage'), ('but', 'vanity'), ('elizabeth', 'smiling'), ('him', 'supercilious'), ('almost', 'have'), ('which', 'lydia'), ('sensible', 'a'), ('hour', 'by')]\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
    "print(len(word_pairs))\n",
    "\n",
    "gram2 = set(word_pairs)\n",
    "print(len(gram2))\n",
    "\n",
    "# Print 20 elements from gram2\n",
    "gram2_iter = iter(gram2)\n",
    "print([next(gram2_iter) for _ in range(20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 4507), ('to', 4243), ('of', 3730), ('and', 3658), ('her', 2225), ('i', 2070), ('a', 2012), ('in', 1937), ('was', 1847), ('she', 1710), ('that', 1594), ('it', 1550), ('not', 1450), ('you', 1428), ('he', 1339), ('his', 1271), ('be', 1260), ('as', 1192), ('had', 1177), ('with', 1100)]\n",
      "4507\n"
     ]
    }
   ],
   "source": [
    "gram1_dict = dict()\n",
    "\n",
    "# Populate 1-gram dictionary\n",
    "for word in words:\n",
    "    if word in gram1_dict:\n",
    "        gram1_dict[word] += 1\n",
    "    else:\n",
    "        gram1_dict[word] = 1 # Start a new entry with 1 count since saw it for the first time.\n",
    "\n",
    "# Turn into a list of (word, count) sorted by count from most to least\n",
    "gram1 = sorted(gram1_dict.items(), key=lambda i: -i[1])\n",
    "\n",
    "# Print top 20 most frequent words\n",
    "print(gram1[:20])\n",
    "\n",
    "def gram1_count(word):\n",
    "    if word in gram1_dict:\n",
    "        return gram1_dict[word]\n",
    "    return 0\n",
    "\n",
    "print(gram1_count('the'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('of', 'the'), 491), (('to', 'be'), 445), (('in', 'the'), 397), (('i', 'am'), 303), (('mr', 'darcy'), 273), (('to', 'the'), 268), (('of', 'her'), 261), (('it', 'was'), 251), (('of', 'his'), 235), (('she', 'was'), 212), (('she', 'had'), 205), (('had', 'been'), 200), (('it', 'is'), 194), (('i', 'have'), 188), (('to', 'her'), 179), (('that', 'he'), 177), (('could', 'not'), 167), (('he', 'had'), 166), (('and', 'the'), 165), (('for', 'the'), 163)]\n",
      "491\n"
     ]
    }
   ],
   "source": [
    "gram2 = dict()\n",
    "\n",
    "# Populate 2-gram dictionary\n",
    "for i in range(len(words)-1):\n",
    "    key = (words[i], words[i+1])\n",
    "    if key in gram2:\n",
    "        gram2[key] += 1\n",
    "    else:\n",
    "        gram2[key] = 1\n",
    "\n",
    "# Turn into a list of (word, count) sorted by count from most to least\n",
    "gram2 = sorted(gram2.items(), key=lambda i: -i[1])\n",
    "\n",
    "# Print top 20 most frequent words\n",
    "print(gram2[:20])\n",
    "\n",
    "gram2_count_cache = {}\n",
    "\n",
    "def gram2_count(word1, word2):\n",
    "    if (word1, word2) not in gram2_count_cache:\n",
    "        gram2_count_cache[(word1, word2)] = 0\n",
    "    else:\n",
    "        return gram2_count_cache[(word1, word2)]\n",
    "    \n",
    "    gram2_count_cache[(word1, word2)] = 0\n",
    "    for ((w1, w2), count) in gram2:\n",
    "        if w2 == word2 and w1 == word1:\n",
    "            gram2_count_cache[(word1, word2)] += count\n",
    "    return gram2_count_cache[(word1, word2)]  \n",
    "\n",
    "print(gram2_count('of', 'the'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('being', 'in', 'love'), ('she', 'was', 'selected'), ('bestowed', 'my', 'father'), ('person', 'well', 'placed'), ('that', 'related', 'to'), ('her', 'own', 'vanity'), ('composure', 'he', 'said'), ('simpleton', 'could', 'she'), ('your', 'favourite', 's'), ('second', 'proposal', 'to'), ('a', 'person', 'may'), ('look', 'or', 'the'), ('most', 'unaffectedly', 'modest'), ('arise', 'from', 'the'), ('but', 'our', 'own'), ('turned', 'out', 'very'), ('debt', 'mr', 'bingley'), ('delight', 'and', 'repeat'), ('looking', 'out', 'of'), ('or', 'any', 'project')]\n",
      "[(('i', 'do', 'not'), 62), (('i', 'am', 'sure'), 62), (('project', 'gutenberg', 'tm'), 57), (('as', 'soon', 'as'), 55), (('she', 'could', 'not'), 50), (('that', 'he', 'had'), 37), (('it', 'would', 'be'), 34), (('in', 'the', 'world'), 34), (('i', 'am', 'not'), 32), (('the', 'project', 'gutenberg'), 31), (('i', 'dare', 'say'), 31), (('it', 'was', 'not'), 30), (('could', 'not', 'be'), 30), (('that', 'he', 'was'), 29), (('mr', 'darcy', 's'), 29), (('that', 'it', 'was'), 28), (('on', 'the', 'subject'), 28), (('of', 'mr', 'darcy'), 27), (('would', 'have', 'been'), 27), (('as', 'well', 'as'), 27)]\n",
      "397\n"
     ]
    }
   ],
   "source": [
    "word_triplets = [(words[i], words[i+1], words[i+2]) for i in range(len(words)-2)]\n",
    "\n",
    "gram3 = set(word_triplets)\n",
    "# Print 20 elements from gram2\n",
    "gram3_iter = iter(gram3)\n",
    "print([next(gram3_iter) for _ in range(20)])\n",
    "\n",
    "gram3 = dict()\n",
    "\n",
    "# Populate 2-gram dictionary\n",
    "for i in range(len(words)-2):\n",
    "    key = (words[i], words[i+1], words[i+2])\n",
    "    if key in gram3:\n",
    "        gram3[key] += 1\n",
    "    else:\n",
    "        gram3[key] = 1\n",
    "\n",
    "# Turn into a list of (word, count) sorted by count from most to least\n",
    "gram3 = sorted(gram3.items(), key=lambda i: -i[1])\n",
    "\n",
    "# Print top 20 most frequent words\n",
    "print(gram3[:20])\n",
    "\n",
    "def gram3_count(word1, word2, word3):\n",
    "    wordcount = 0\n",
    "    for ((w1, w2, w3), count) in gram3:\n",
    "        if w2 == word2 and w1 == word1 and w3 == word3:\n",
    "            return count\n",
    "    return 0 \n",
    "\n",
    "gram3_count_cache = {}\n",
    "def gram3_first_2_count(word1, word2):\n",
    "    if (word1, word2) not in gram3_count_cache:\n",
    "        gram3_count_cache[(word1, word2)] = 0\n",
    "    else: \n",
    "        return gram3_count_cache[(word1, word2)]\n",
    "    \n",
    "    for ((w1, w2, w3), count) in gram3:\n",
    "        if w1 == word1 and w2 == word2:\n",
    "            gram3_count_cache[(word1, word2)] += count\n",
    "    \n",
    "    return gram3_count_cache[(word1, word2)] \n",
    "\n",
    "print(gram3_first_2_count('in', 'the'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next-word prediction\n",
    "\n",
    "Take a random word from a corpus and try to build next word as a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delighted\n"
     ]
    }
   ],
   "source": [
    "start_word = words[int(len(words)/4)]\n",
    "print(start_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm:\n",
    "\n",
    "1. Pick a random word\n",
    "2. Find most frequent n-gram with this word as a first one\n",
    "3. Pick 2nd word from that n-gram\n",
    "4. repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word: delighted\n",
      "2-gram sentence: \"\n",
      "delighted with the whole party were to be so much as to be so much as to be so much \"\n"
     ]
    }
   ],
   "source": [
    "def get2GramSentence(word, n = 50):\n",
    "    for i in range(n):\n",
    "        print(word, end=' ')\n",
    "        # Find Next word\n",
    "        word = next((element[0][1] for element in gram2 if element[0][0] == word), None)\n",
    "        if not word:\n",
    "            break\n",
    "\n",
    "word = start_word\n",
    "print(\"Start word: %s\" % word)\n",
    "\n",
    "print(\"2-gram sentence: \\\"\",)\n",
    "get2GramSentence(word, 20)\n",
    "print(\"\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted choices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def weighted_choice(choices):\n",
    "   total = sum(w for c, w in choices)\n",
    "   r = random.uniform(0, total)\n",
    "   upto = 0\n",
    "   for c, w in choices:\n",
    "      if upto + w > r:\n",
    "         return c\n",
    "      upto += w\n",
    "    \n",
    "def get2GramSentenceRandom(word, n = 50):\n",
    "    for i in range(n):\n",
    "        print(word, end=' ')\n",
    "        # Get all possible elements ((first word, second word), frequency)\n",
    "        choices = [element for element in gram2 if element[0][0] == word]\n",
    "        if not choices:\n",
    "            break\n",
    "        \n",
    "        # Choose a pair with weighted probability from the choice list\n",
    "        word = weighted_choice(choices)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start word: and\n",
      "2-gram sentence: \" and the library as i might say for the happy to be angry that the pause awkward as his sister s performance mixing with an ante chamber to your situation the wild that miss lucas and made her dirty weather should be out miss de bourgh are not a partner \"\n",
      "Start word: he\n",
      "2-gram sentence: \" he must cease to the library he to satisfy us if she to theirs i understand it and mrs collins and soon as sensibly and she could be supposed might yet had known this could would certainly had arisen repeated discussion of her father then gathered round and dull for \"\n",
      "Start word: she\n",
      "2-gram sentence: \" she likes to go to be inclined to prove that abominable mr collins at her room in essentials oh my fair opportunity of the bad as they must have delivered his behaviour was out said lady lucas whom she was never heard that this to make the instrument she could \"\n",
      "Start word: when\n",
      "2-gram sentence: \" when all were not be sorry replied she stared at civility i should come there are too much more natural self importance which had been long survive without any flirting i shall probably might yet be known at stoke if i am not be in your mother in the order \"\n",
      "Start word: john\n",
      "2-gram sentence: \" john with only ten words of mr bennet they had ever felt it would have prevented by visiting and his pride and when i am most iniquitous affair said her as soon lost but i should be congenial with a fool s engrossing him amends for it so dull to \"\n",
      "Start word: never\n",
      "2-gram sentence: \" never come to put herself near one equally lively imagination a young olive branch but they instantly to avoid seeing mr collins had chosen it as you possess the sight of cautiousness to mrs bennet after assuring his proposals she had not unlikely to lydia s side of the steps \"\n",
      "Start word: i\n",
      "2-gram sentence: \" i am a connection was not wish of both chapter during the improvements it that i am inclined to submit and are by his behaviour of bingley and come back jane and teach only confused as often created a short time and so much better than his affection of ease \"\n",
      "Start word: how\n",
      "2-gram sentence: \" how strange it is come for some time when we talked on the principal object of her fellow s sisters soon able to the first coming to disguise of derbyshire so very likely to be pointed out as they now say by his manner which convinced her thoughts perhaps i \"\n"
     ]
    }
   ],
   "source": [
    "for word in ['and', 'he', 'she', 'when', 'john', 'never', 'i', 'how']:\n",
    "    print(\"Start word: %s\" % word)\n",
    "\n",
    "    print(\"2-gram sentence: \\\"\", end=\" \")\n",
    "    get2GramSentenceRandom(word, 50)\n",
    "    print(\"\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural N-Gram Model\n",
    "\n",
    "1. X: Build representation of words as contactenated sequence (current  word, K previous, and count n-grams ending on current word)\n",
    "2. Y: log P(word|history)\n",
    "2. No softmax\n",
    "\n",
    "fnng(w[i]..)=log P(w1|w[i-1..,i-k], c[i])\n",
    "c[u]=Count(w[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "num_cores = 16\n",
    "\n",
    "CPU = True\n",
    "GPU = False\n",
    "\n",
    "if GPU:\n",
    "    num_GPU = 1\n",
    "    num_CPU = 1\n",
    "if CPU:\n",
    "    num_CPU = 1\n",
    "    num_GPU = 0\n",
    "\n",
    "config = tensorflow.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "session = tensorflow.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surov\\Miniconda3\\envs\\tf\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# word embeddings\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "embedding_size = 128\n",
    "\n",
    "v2vec = Word2Vec(sentences, size=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: (105491, 2, 128)\tC: (105491, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import os.path\n",
    "\n",
    "def get_embedding(word):\n",
    "    if word not in v2vec.wv:\n",
    "        return np.zeros(embedding_size)\n",
    "    return v2vec.wv[word]\n",
    "\n",
    "def prepare_nn_inputs():\n",
    "    pbar = tqdm.tqdm(total=len(gram3))\n",
    "    \n",
    "    W = []\n",
    "    C = []\n",
    "    Y = []\n",
    "\n",
    "    for (word1, word2, word3), count in gram3:\n",
    "        pbar.update()\n",
    "\n",
    "        w = np.zeros((L, embedding_size))\n",
    "        w[0] = get_embedding(word1)\n",
    "        w[1] = get_embedding(word2)\n",
    "        W.append(w)\n",
    "\n",
    "        c = np.zeros((3, 2))\n",
    "        c[0, 0] = 0\n",
    "        c[0, 1] = gram1_count(word1)\n",
    "        c[1, 0] = gram2_count(word1, word2)\n",
    "        c[1, 1] = gram1_count(word2)  \n",
    "        c[2, 0] = gram2_count(word2, word3)\n",
    "        c[2, 1] = gram1_count(word3)\n",
    "\n",
    "        C.append(c)\n",
    "\n",
    "        y = np.log(count / gram3_first_2_count(word1, word2))\n",
    "        Y.append(y)\n",
    "    \n",
    "    W = np.array(W)\n",
    "    C = np.array(C)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    np.save(\"W.npy\", W)\n",
    "    np.save(\"C.npy\", C)\n",
    "    np.save(\"Y.npy\", Y)\n",
    "\n",
    "    pbar.close()\n",
    "    return (W, C, Y)\n",
    "\n",
    "N = 2 # n-gram output depth\n",
    "L = 2 # n-gram input statistics depth\n",
    "\n",
    "W = []\n",
    "C = []\n",
    "Y = []\n",
    "\n",
    "if not os.path.isfile(\"W.npy\"):\n",
    "    (_W, _C, _Y) = prepare_nn_inputs()\n",
    "    W = _W\n",
    "    C = _C\n",
    "    Y = _Y\n",
    "else:\n",
    "    W = np.load(\"W.npy\")\n",
    "    C = np.load(\"C.npy\")\n",
    "    Y = np.load(\"Y.npy\")\n",
    "\n",
    "print(\"W: %s\\tC: %s\" % (W.shape, C.shape))\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, concatenate, Activation, Reshape, Flatten\n",
    "\n",
    "\n",
    "def build_nn_gram_model():\n",
    "    input1 = Input(shape=(L, embedding_size), name='embeddings_input')\n",
    "    reshape1 = Reshape((-1, L*embedding_size))(input1)\n",
    "    flatten1 = Flatten()(reshape1)\n",
    "    x1 = Dense(128, activation='relu')(flatten1)\n",
    "    \n",
    "    input2 = Input(shape=((L+1), N), name='counts_input')\n",
    "    reshape2 = Reshape((-1, (L+1)*N))(input2)\n",
    "    flatten2 = Flatten()(reshape2)\n",
    "    x2 = Dense(32, activation='relu')(flatten2)\n",
    "    \n",
    "    x3 = concatenate([x1, x2])\n",
    "    y = Dense(1, activation='relu', name='logp')(x1)\n",
    "\n",
    "    model = Model(inputs=[input1], outputs=y)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# this one is OK\n",
    "def build_simple_sequential_model():\n",
    "    model = Sequential()\n",
    "    model.add(Reshape((-1, L*embedding_size), input_shape=(L, embedding_size), name='embeddings'))\n",
    "    model.add(Flatten()) \n",
    "    model.add(Dense(1, activation='relu', name='logp'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error' ,optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_simple_functional_model():\n",
    "    input1 = Input(shape=(L, embedding_size), name='embeddings_input')\n",
    "    reshape1 = Reshape((-1, L*embedding_size))(input1)\n",
    "    flatten1 = Flatten()(reshape1)\n",
    "    y = Dense(1, activation='relu', name='logp')(flatten1)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=y)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sequential Model:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embeddings (Reshape)         (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_49 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "logp (Dense)                 (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 257\n",
      "Trainable params: 257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "simple_model_sequiential = build_simple_sequential_model()\n",
    "print(\"\\n\\nSequential Model:\")\n",
    "print(simple_model_sequiential.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Funcational Model:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embeddings (Reshape)         (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_45 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "logp (Dense)                 (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 257\n",
      "Trainable params: 257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "simple_model_functional = build_simple_functional_model()\n",
    "print(\"\\n\\nFuncational Model:\")\n",
    "print(simple_model_sequiential.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Funcational Model:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embeddings (InputLayer)      (None, 2, 128)            0         \n",
      "_________________________________________________________________\n",
      "reshape_40 (Reshape)         (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_47 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "logp (Dense)                 (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 33,025\n",
      "Trainable params: 33,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_nn_gram_model()\n",
    "print(\"\\n\\nFuncational Model:\")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'embeddings_input': W,\n",
    "    'counts_input': C,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 94941 samples, validate on 10550 samples\n",
      "Epoch 1/1\n",
      "94941/94941 [==============================] - 1s 8us/step - loss: 3.8451 - val_loss: 3.5125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19a84dca400>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model_functional.fit(params, Y, epochs=1, validation_split=0.1, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 94941 samples, validate on 10550 samples\n",
      "Epoch 1/1\n",
      "94941/94941 [==============================] - 1s 15us/step - loss: 3.8462 - val_loss: 3.5125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19a84dca128>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model_sequiential.fit(params, Y, epochs=1, validation_split=0.1, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No data provided for \"embeddings\". Need data for each key in: ['embeddings']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m             ]\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'DataFrame'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m             ]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'embeddings'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-48659f71e442>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    951\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 749\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m     75\u001b[0m             raise ValueError('No data provided for \"' + e.args[0] +\n\u001b[0;32m     76\u001b[0m                              \u001b[1;34m'\". Need data '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m                              'for each key in: ' + str(names))\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No data provided for \"embeddings\". Need data for each key in: ['embeddings']"
     ]
    }
   ],
   "source": [
    "model.fit(params, Y, epochs=1, validation_split=0.1, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
